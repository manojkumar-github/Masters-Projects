1) Why are you using CNN instead of RNN for text classification?
2) Language Models
3) BackPropogation, Vanishing/Increasing gradient Problem (How do you deal with it, Early cutting), Epochs, cycles, iterations
4) Transfer Learning, Freeze weights and how do you do?
5) Active Learning
6) RNN and CNN Architectures
7) Embeddings
8) Tensorflow visuaizations
9) Training on GPUs
10) LSTM vs GRU
11) CNN padding, and its layers
12) CharLevel vs Word Level vs Doc Level Embeddings. How do you implement? 
13) How do you preprocess text data for RNN and CNN?
14) Shapes and coversions of shapes at each layers?
15) Dropout and Activations(ReLu)
16) Argmax and Metrics
17) Google DeepLearning Interview Questions of LSTM, CNN and other new architectures
18) Universal Embeddings
19) Gradient Descent/ Gradient Ascent/ Stochastic Gradient Descent ...one step at a time, large steps
20) Concave and Convex functions, Global Minima and Global Maxima
21) Adam and all other optimizers
22) Weights Initializtion
23) Theis and weights dropping and light weight
24) Best cost function
25) End to End NN architecture for a problem - How many layers and what parameters do you start with and why?
26) Metrics
27) What are convolutional kernels?
28) Important research papers
29) Combination of CNN and RNNs?
30) Iterating in minbatches and its advantages in gradient descent? 
31) Batch Normalization and Regularization? batch norm we need to know if this is train or the test phase
32) Are votes shared in CNN or RNN? What is the advantage of sharing of votes?
33) Momentum, Learning Rate and Optimizers When is when and why? 
34) Variants of RNN, LSTM and CNNs (BiDirectional, Dynamic)
35) Augmenting models
36) Cyclical Learning Rates
37) Understand the differences between character and word emebeddings
38) Loss functions when is when
39) VAE
40) Implement NN, Dropout and Backpropogation from scratch
41) Understand vocabulary and dictionary and embedding lookup in the context of RNN and CNN
42) Encoder/Decoder network
43) Bidirectional RNN (Past + future Inputs and two RNN are stacked on each other and the decision is based on both of the states) ------ Deep Bidirectional RNNs use multiple layers at each time step


TensorFlow:
1) Variable Scope
